{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'glove'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7588023582cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mglove\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'glove'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_contrib.layers.crf import CRF, crf_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from glove import Corpus, Glove\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import operator\n",
    "#import spacy #spacy multilanguage\n",
    "#nlp = spacy.load('xx')\n",
    "from spacy.lang.xx import MultiLanguage\n",
    "nlp = MultiLanguage() #pre-trained model NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readline(filename):\n",
    "    \"\"\"\n",
    "        read file\n",
    "        return\n",
    "        format [ ['segelas', 'TYPES'], ['douni', 'PRODUCT], ['parfum', 'PRODUCT'], ['collection', 'PRODUCT'], ['fusion', 'PRODUCT']]\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for i in f:\n",
    "            if(len(i[:-1]) < 2) or i.startswith(\"-DOCSTART-\"):\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "            splits = i[:-1].split(\",\")\n",
    "            sentence.append([splits[0], splits[-1]])\n",
    "        \n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMatrices(sentences, word2Idx, label2Idx, char2Idx):\n",
    "    \"\"\"\n",
    "    Get the matrices for every dataset. Different with normal NER, we do not use casing-features here, because the input \n",
    "    from ASR would be case insensitive\n",
    "        sentences(list) : list of list of words from the input.\n",
    "        word2Idx(dict)   : Word and its index\n",
    "        label2Idx(dict)   : Label and its index\n",
    "        char2Idx(dict)   : Characters and its index\n",
    "        \n",
    "    Output:\n",
    "        dataset(matrix)   :Matrix containing all of the features.\n",
    "    \"\"\"\n",
    "    unknownIdx = word2Idx[\"UNK\"]\n",
    "    paddingIdx = word2Idx[\"PAD\"]\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    word_count = 0\n",
    "    unknown_word_count = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        word_indices = []\n",
    "        char_indices = []\n",
    "        label_indices = []\n",
    "        for word, char, label in sentence:\n",
    "            word_count += 1\n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]\n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "                unknown_word_count += 1\n",
    "            \n",
    "            charIdx = []\n",
    "            for x in char:\n",
    "                try:\n",
    "                    charIdx.append(char2Idx[x])\n",
    "                except KeyError:\n",
    "                    charIdx.append(char2Idx[\"UNK\"])\n",
    "            \n",
    "            word_indices.append(wordIdx)\n",
    "            char_indices.append(charIdx)\n",
    "            label_indices.append(label2Idx[label])\n",
    "            \n",
    "        dataset.append([word_indices, char_indices, label_indices])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCharInformation(sentences):\n",
    "    \"\"\"\n",
    "    Split the word in the sentences from 1 dataset into list of characters:\n",
    "        ex: [[\"S\",\"e\",\"l\",\"a\",\"m\",\"a\",\"t\"],[\"P\",\"a\",\"g\",\"i\"]]\n",
    "    Input:\n",
    "        sentences(list)   : The dataset\n",
    "    \"\"\"    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, data in enumerate(sentence):\n",
    "            chars = [i for i in data[0]]\n",
    "            sentences[i][j] = [data[0], chars, data[1]] ## data[0] is token, chars is chars of token, data[1] is label/entity\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sentences):\n",
    "    \"\"\"\n",
    "    Because the batches already with same length, we are not padding it anymore. We padding the characters instead\n",
    "        ex: [[\"S\",\"e\",\"l\",\"a\",\"m\",\"a\",\"t\"],[\"P\",\"a\",\"g\",\"i\",\"<PAD>\",\"<PAD>\",\"<PAD>\"]]\n",
    "        \n",
    "    Input:\n",
    "        sentence(list)   :The dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    maxwordlength = 10 #10 character max in 1 word\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        sentences[i][1] = pad_sequences(sentences[i][1], maxwordlength, padding='pre',truncating='post')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBatches(data):\n",
    "    \"\"\"\n",
    "    Create the batches for train_on_batch keras.\n",
    "    Input:\n",
    "        data(list)   :The dataset\n",
    "    \"\"\"\n",
    "    l = []\n",
    "    for i in data:\n",
    "        l.append(len(i[0]))\n",
    "    l = set(l)  ## kata\n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    \n",
    "    for i in l:\n",
    "        for batch in data:\n",
    "            if(len(batch[0]) == i):\n",
    "                batches.append(batch)\n",
    "                z+=1\n",
    "        batch_len.append(z)\n",
    "    return batches, batch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(dataset,batch_len):\n",
    "    \"\"\"\n",
    "    Generator for dataset to be yielded during training\n",
    "    Input:\n",
    "        Dataset(list) : The dataset\n",
    "        batch_len(list) :Start and end batch_len\n",
    "    Output:\n",
    "        Array of [labels, tokens, casing, char] features\n",
    "    \"\"\"    \n",
    "    start = 0\n",
    "    for i in batch_len:\n",
    "        tokens = []\n",
    "        char = []\n",
    "        labels = []\n",
    "        data = dataset[start:i]\n",
    "        start = i\n",
    "        for dt in data:\n",
    "            t,ch,l = dt\n",
    "            l = np.expand_dims(l, -1)\n",
    "            tokens.append(t)\n",
    "            char.append(ch)\n",
    "            labels.append(l)\n",
    "        yield np.asarray(labels), np.asarray(tokens),np.asarray(char) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict data\n",
    "def tag_dataset(dataset):\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "    b = Progbar(len(dataset))\n",
    "    for i, data in enumerate(dataset):\n",
    "        tokens, char, labels = data\n",
    "        tokens = np.asarray([tokens])\n",
    "        char = np.asarray([char])\n",
    "        \n",
    "        pred = model.predict([tokens, char], verbose = False)[0]\n",
    "        pred = pred.argmax(axis = -1) #Predict the classes\n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "        b.update(i)\n",
    "    b.update(i+1)\n",
    "    \n",
    "    return predLabels, correctLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Open the file needed and Convert the word into list of chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences = readline(\"../data/clean/dataset.csv\")\n",
    "#trainSentences = readline(\"../data/clean/data_full_product.csv\")\n",
    "trainSentences = addCharInformation(trainSentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the Label from the dataset, and convert it to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_set = set()\n",
    "words = {}\n",
    "for sentence in trainSentences:\n",
    "    for token, char, label in sentence:\n",
    "        label_set.add(label)\n",
    "        words[token.lower()] = True\n",
    "\n",
    "label2idx = {v:k for k,v in enumerate(sorted(label_set))}\n",
    "idx2label = {v:k for k,v in label2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. GloVe Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_word2idx_glove = open(\"../data/pickle_file/word2idx_glove.pkl\",\"rb\")\n",
    "word2idx_glove = pickle.load(pickle_word2idx_glove)\n",
    "word2Idx = {i[0]:i[1] for i in word2idx_glove}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file_path = '../model/glove_embedding.txt'\n",
    "\n",
    "glove = Glove()\n",
    "glove = glove.load(embedding_file_path)\n",
    "wordEmbeddings = glove.word_vectors\n",
    "n_dim = glove.word_vectors.shape[1]\n",
    "\n",
    "word2Idx.update({'PAD': len(word2Idx)})\n",
    "wordEmbeddings = np.concatenate((wordEmbeddings, np.zeros((1, n_dim))))\n",
    "word2Idx.update({'UNK': len(word2Idx)})\n",
    "wordEmbeddings = np.concatenate((wordEmbeddings, np.random.uniform(-0.25*10**-8, 0.25*10**-8,n_dim).reshape((1,n_dim))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 150)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordEmbeddings[:5].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Get list of characters and its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2Idx = {\"PAD\":0, \"UNK\":1}\n",
    "for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|\":\n",
    "    char2Idx[c] = len(char2Idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set = createMatrices(trainSentences, \n",
    "               word2Idx = word2Idx, \n",
    "               label2Idx = label2idx,\n",
    "               char2Idx = char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = padding(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(train_set, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_batch, train_batch_len = createBatches(X_train)\n",
    "test_batch, test_batch_len = createBatches(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[594, 32611, 54776, 80868, 138532, 202246, 241493, 253787, 257868, 260895]\n",
      "[150, 8133, 13740, 20284, 34739, 50606, 60395, 63476, 64479, 65224]\n"
     ]
    }
   ],
   "source": [
    "print(train_batch_len)\n",
    "print(test_batch_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256, 512, 594, 768, 1024, 1280, 1536, 1792, 2048, 2304, 2560, 2816, 3072, 3328, 3584, 3840, 4096, 4352, 4608, 4864, 5120, 5376, 5632, 5888, 6144, 6400, 6656, 6912, 7168, 7424, 7680, 7936, 8192, 8448, 8704, 8960, 9216, 9472, 9728, 9984, 10240, 10496, 10752, 11008, 11264, 11520, 11776, 12032, 12288, 12544, 12800, 13056, 13312, 13568, 13824, 14080, 14336, 14592, 14848, 15104, 15360, 15616, 15872, 16128, 16384, 16640, 16896, 17152, 17408, 17664, 17920, 18176, 18432, 18688, 18944, 19200, 19456, 19712, 19968, 20224, 20480, 20736, 20992, 21248, 21504, 21760, 22016, 22272, 22528, 22784, 23040, 23296, 23552, 23808, 24064, 24320, 24576, 24832, 25088, 25344, 25600, 25856, 26112, 26368, 26624, 26880, 27136, 27392, 27648, 27904, 28160, 28416, 28672, 28928, 29184, 29440, 29696, 29952, 30208, 30464, 30720, 30976, 31232, 31488, 31744, 32000, 32256, 32512, 32611, 32768, 33024, 33280, 33536, 33792, 34048, 34304, 34560, 34816, 35072, 35328, 35584, 35840, 36096, 36352, 36608, 36864, 37120, 37376, 37632, 37888, 38144, 38400, 38656, 38912, 39168, 39424, 39680, 39936, 40192, 40448, 40704, 40960, 41216, 41472, 41728, 41984, 42240, 42496, 42752, 43008, 43264, 43520, 43776, 44032, 44288, 44544, 44800, 45056, 45312, 45568, 45824, 46080, 46336, 46592, 46848, 47104, 47360, 47616, 47872, 48128, 48384, 48640, 48896, 49152, 49408, 49664, 49920, 50176, 50432, 50688, 50944, 51200, 51456, 51712, 51968, 52224, 52480, 52736, 52992, 53248, 53504, 53760, 54016, 54272, 54528, 54776, 54784, 55040, 55296, 55552, 55808, 56064, 56320, 56576, 56832, 57088, 57344, 57600, 57856, 58112, 58368, 58624, 58880, 59136, 59392, 59648, 59904, 60160, 60416, 60672, 60928, 61184, 61440, 61696, 61952, 62208, 62464, 62720, 62976, 63232, 63488, 63744, 64000, 64256, 64512, 64768, 65024, 65280, 65536, 65792, 66048, 66304, 66560, 66816, 67072, 67328, 67584, 67840, 68096, 68352, 68608, 68864, 69120, 69376, 69632, 69888, 70144, 70400, 70656, 70912, 71168, 71424, 71680, 71936, 72192, 72448, 72704, 72960, 73216, 73472, 73728, 73984, 74240, 74496, 74752, 75008, 75264, 75520, 75776, 76032, 76288, 76544, 76800, 77056, 77312, 77568, 77824, 78080, 78336, 78592, 78848, 79104, 79360, 79616, 79872, 80128, 80384, 80640, 80868, 80896, 81152, 81408, 81664, 81920, 82176, 82432, 82688, 82944, 83200, 83456, 83712, 83968, 84224, 84480, 84736, 84992, 85248, 85504, 85760, 86016, 86272, 86528, 86784, 87040, 87296, 87552, 87808, 88064, 88320, 88576, 88832, 89088, 89344, 89600, 89856, 90112, 90368, 90624, 90880, 91136, 91392, 91648, 91904, 92160, 92416, 92672, 92928, 93184, 93440, 93696, 93952, 94208, 94464, 94720, 94976, 95232, 95488, 95744, 96000, 96256, 96512, 96768, 97024, 97280, 97536, 97792, 98048, 98304, 98560, 98816, 99072, 99328, 99584, 99840, 100096, 100352, 100608, 100864, 101120, 101376, 101632, 101888, 102144, 102400, 102656, 102912, 103168, 103424, 103680, 103936, 104192, 104448, 104704, 104960, 105216, 105472, 105728, 105984, 106240, 106496, 106752, 107008, 107264, 107520, 107776, 108032, 108288, 108544, 108800, 109056, 109312, 109568, 109824, 110080, 110336, 110592, 110848, 111104, 111360, 111616, 111872, 112128, 112384, 112640, 112896, 113152, 113408, 113664, 113920, 114176, 114432, 114688, 114944, 115200, 115456, 115712, 115968, 116224, 116480, 116736, 116992, 117248, 117504, 117760, 118016, 118272, 118528, 118784, 119040, 119296, 119552, 119808, 120064, 120320, 120576, 120832, 121088, 121344, 121600, 121856, 122112, 122368, 122624, 122880, 123136, 123392, 123648, 123904, 124160, 124416, 124672, 124928, 125184, 125440, 125696, 125952, 126208, 126464, 126720, 126976, 127232, 127488, 127744, 128000, 128256, 128512, 128768, 129024, 129280, 129536, 129792, 130048, 130304, 130560, 130816, 131072, 131328, 131584, 131840, 132096, 132352, 132608, 132864, 133120, 133376, 133632, 133888, 134144, 134400, 134656, 134912, 135168, 135424, 135680, 135936, 136192, 136448, 136704, 136960, 137216, 137472, 137728, 137984, 138240, 138496, 138532, 138752, 139008, 139264, 139520, 139776, 140032, 140288, 140544, 140800, 141056, 141312, 141568, 141824, 142080, 142336, 142592, 142848, 143104, 143360, 143616, 143872, 144128, 144384, 144640, 144896, 145152, 145408, 145664, 145920, 146176, 146432, 146688, 146944, 147200, 147456, 147712, 147968, 148224, 148480, 148736, 148992, 149248, 149504, 149760, 150016, 150272, 150528, 150784, 151040, 151296, 151552, 151808, 152064, 152320, 152576, 152832, 153088, 153344, 153600, 153856, 154112, 154368, 154624, 154880, 155136, 155392, 155648, 155904, 156160, 156416, 156672, 156928, 157184, 157440, 157696, 157952, 158208, 158464, 158720, 158976, 159232, 159488, 159744, 160000, 160256, 160512, 160768, 161024, 161280, 161536, 161792, 162048, 162304, 162560, 162816, 163072, 163328, 163584, 163840, 164096, 164352, 164608, 164864, 165120, 165376, 165632, 165888, 166144, 166400, 166656, 166912, 167168, 167424, 167680, 167936, 168192, 168448, 168704, 168960, 169216, 169472, 169728, 169984, 170240, 170496, 170752, 171008, 171264, 171520, 171776, 172032, 172288, 172544, 172800, 173056, 173312, 173568, 173824, 174080, 174336, 174592, 174848, 175104, 175360, 175616, 175872, 176128, 176384, 176640, 176896, 177152, 177408, 177664, 177920, 178176, 178432, 178688, 178944, 179200, 179456, 179712, 179968, 180224, 180480, 180736, 180992, 181248, 181504, 181760, 182016, 182272, 182528, 182784, 183040, 183296, 183552, 183808, 184064, 184320, 184576, 184832, 185088, 185344, 185600, 185856, 186112, 186368, 186624, 186880, 187136, 187392, 187648, 187904, 188160, 188416, 188672, 188928, 189184, 189440, 189696, 189952, 190208, 190464, 190720, 190976, 191232, 191488, 191744, 192000, 192256, 192512, 192768, 193024, 193280, 193536, 193792, 194048, 194304, 194560, 194816, 195072, 195328, 195584, 195840, 196096, 196352, 196608, 196864, 197120, 197376, 197632, 197888, 198144, 198400, 198656, 198912, 199168, 199424, 199680, 199936, 200192, 200448, 200704, 200960, 201216, 201472, 201728, 201984, 202240, 202246, 202496, 202752, 203008, 203264, 203520, 203776, 204032, 204288, 204544, 204800, 205056, 205312, 205568, 205824, 206080, 206336, 206592, 206848, 207104, 207360, 207616, 207872, 208128, 208384, 208640, 208896, 209152, 209408, 209664, 209920, 210176, 210432, 210688, 210944, 211200, 211456, 211712, 211968, 212224, 212480, 212736, 212992, 213248, 213504, 213760, 214016, 214272, 214528, 214784, 215040, 215296, 215552, 215808, 216064, 216320, 216576, 216832, 217088, 217344, 217600, 217856, 218112, 218368, 218624, 218880, 219136, 219392, 219648, 219904, 220160, 220416, 220672, 220928, 221184, 221440, 221696, 221952, 222208, 222464, 222720, 222976, 223232, 223488, 223744, 224000, 224256, 224512, 224768, 225024, 225280, 225536, 225792, 226048, 226304, 226560, 226816, 227072, 227328, 227584, 227840, 228096, 228352, 228608, 228864, 229120, 229376, 229632, 229888, 230144, 230400, 230656, 230912, 231168, 231424, 231680, 231936, 232192, 232448, 232704, 232960, 233216, 233472, 233728, 233984, 234240, 234496, 234752, 235008, 235264, 235520, 235776, 236032, 236288, 236544, 236800, 237056, 237312, 237568, 237824, 238080, 238336, 238592, 238848, 239104, 239360, 239616, 239872, 240128, 240384, 240640, 240896, 241152, 241408, 241493, 241664, 241920, 242176, 242432, 242688, 242944, 243200, 243456, 243712, 243968, 244224, 244480, 244736, 244992, 245248, 245504, 245760, 246016, 246272, 246528, 246784, 247040, 247296, 247552, 247808, 248064, 248320, 248576, 248832, 249088, 249344, 249600, 249856, 250112, 250368, 250624, 250880, 251136, 251392, 251648, 251904, 252160, 252416, 252672, 252928, 253184, 253440, 253696, 253787, 253952, 254208, 254464, 254720, 254976, 255232, 255488, 255744, 256000, 256256, 256512, 256768, 257024, 257280, 257536, 257792, 257868, 258048, 258304, 258560, 258816, 259072, 259328, 259584, 259840, 260096, 260352, 260608, 260864, 260895]\n",
      "[150, 256, 512, 768, 1024, 1280, 1536, 1792, 2048, 2304, 2560, 2816, 3072, 3328, 3584, 3840, 4096, 4352, 4608, 4864, 5120, 5376, 5632, 5888, 6144, 6400, 6656, 6912, 7168, 7424, 7680, 7936, 8133, 8192, 8448, 8704, 8960, 9216, 9472, 9728, 9984, 10240, 10496, 10752, 11008, 11264, 11520, 11776, 12032, 12288, 12544, 12800, 13056, 13312, 13568, 13740, 13824, 14080, 14336, 14592, 14848, 15104, 15360, 15616, 15872, 16128, 16384, 16640, 16896, 17152, 17408, 17664, 17920, 18176, 18432, 18688, 18944, 19200, 19456, 19712, 19968, 20224, 20284, 20480, 20736, 20992, 21248, 21504, 21760, 22016, 22272, 22528, 22784, 23040, 23296, 23552, 23808, 24064, 24320, 24576, 24832, 25088, 25344, 25600, 25856, 26112, 26368, 26624, 26880, 27136, 27392, 27648, 27904, 28160, 28416, 28672, 28928, 29184, 29440, 29696, 29952, 30208, 30464, 30720, 30976, 31232, 31488, 31744, 32000, 32256, 32512, 32768, 33024, 33280, 33536, 33792, 34048, 34304, 34560, 34739, 34816, 35072, 35328, 35584, 35840, 36096, 36352, 36608, 36864, 37120, 37376, 37632, 37888, 38144, 38400, 38656, 38912, 39168, 39424, 39680, 39936, 40192, 40448, 40704, 40960, 41216, 41472, 41728, 41984, 42240, 42496, 42752, 43008, 43264, 43520, 43776, 44032, 44288, 44544, 44800, 45056, 45312, 45568, 45824, 46080, 46336, 46592, 46848, 47104, 47360, 47616, 47872, 48128, 48384, 48640, 48896, 49152, 49408, 49664, 49920, 50176, 50432, 50606, 50688, 50944, 51200, 51456, 51712, 51968, 52224, 52480, 52736, 52992, 53248, 53504, 53760, 54016, 54272, 54528, 54784, 55040, 55296, 55552, 55808, 56064, 56320, 56576, 56832, 57088, 57344, 57600, 57856, 58112, 58368, 58624, 58880, 59136, 59392, 59648, 59904, 60160, 60395, 60416, 60672, 60928, 61184, 61440, 61696, 61952, 62208, 62464, 62720, 62976, 63232, 63476, 63488, 63744, 64000, 64256, 64479, 64512, 64768, 65024, 65224]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_batch_size = []\n",
    "test_batch_size = []\n",
    "\n",
    "## Change training batch size\n",
    "for val in range(batch_size, max(train_batch_len), batch_size):\n",
    "    train_batch_size.append(val)\n",
    "for val in train_batch_len:\n",
    "    train_batch_size.append(val)\n",
    "    \n",
    "train_batch_size.sort()\n",
    "\n",
    "## Change testing batch size\n",
    "for val in range(batch_size, max(test_batch_len), batch_size):\n",
    "    test_batch_size.append(val)\n",
    "for val in test_batch_len:\n",
    "    test_batch_size.append(val)\n",
    "\n",
    "test_batch_size.sort()\n",
    "\n",
    "# print\n",
    "print(train_batch_size)\n",
    "print(test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dropout, TimeDistributed, Conv1D, Dense, multiply, CuDNNLSTM, GlobalAveragePooling1D\n",
    "from keras.layers import Concatenate, MaxPooling1D, Flatten, Bidirectional, LSTM\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import plot_model,Progbar\n",
    "from keras_multi_head import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxwordlength = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "words_input = Input(shape = (None, ), dtype = np.int32, name = 'words_input')\n",
    "words = Embedding(input_dim = wordEmbeddings.shape[0],\n",
    "                  output_dim = wordEmbeddings.shape[1],\n",
    "                  weights = [wordEmbeddings],\n",
    "                  trainable = False)(words_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_input = Input(shape = (None, maxwordlength, ), name = 'char_input')\n",
    "x = TimeDistributed(Embedding(input_dim = len(char2Idx),\n",
    "                      output_dim = 32,\n",
    "                      embeddings_initializer = RandomUniform()))(char_input)\n",
    "\n",
    "conv1 = TimeDistributed(Conv1D(filters = 5,\n",
    "                           kernel_size = 1,\n",
    "                           padding = 'same',\n",
    "                           activation = 'tanh',\n",
    "                           strides = 1))(x)\n",
    "maxpool1 = TimeDistributed(GlobalAveragePooling1D())(conv1)\n",
    "\n",
    "conv2 = TimeDistributed(Conv1D(filters = 5,\n",
    "                           kernel_size = 2,\n",
    "                           padding = 'same',\n",
    "                           activation = 'tanh',\n",
    "                           strides = 2))(x)\n",
    "maxpool2 = TimeDistributed(GlobalAveragePooling1D())(conv2)\n",
    "\n",
    "conv3 = TimeDistributed(Conv1D(filters = 5,\n",
    "                           kernel_size = 3,\n",
    "                           padding = 'same',\n",
    "                           activation = 'tanh',\n",
    "                           strides = 3))(x)\n",
    "maxpool3 = TimeDistributed(GlobalAveragePooling1D())(conv3)\n",
    "\n",
    "conv4 = TimeDistributed(Conv1D(filters = 5,\n",
    "                           kernel_size = 3,\n",
    "                           padding = 'same',\n",
    "                           activation = 'tanh',\n",
    "                           strides = 4))(x)\n",
    "maxpool4 = TimeDistributed(GlobalAveragePooling1D())(conv4)\n",
    "\n",
    "conv5 = TimeDistributed(Conv1D(filters = 5,\n",
    "                           kernel_size = 4,\n",
    "                           padding = 'same',\n",
    "                           activation = 'tanh',\n",
    "                           strides = 5))(x)\n",
    "maxpool5 = TimeDistributed(GlobalAveragePooling1D())(conv5)\n",
    "\n",
    "char_lstm = TimeDistributed(Bidirectional(CuDNNLSTM(128)))(x)\n",
    "\n",
    "concat = Concatenate()([char_lstm, maxpool1, maxpool2, maxpool3, maxpool4, maxpool5])\n",
    "char = TimeDistributed(Flatten())(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = Concatenate()([words, char])\n",
    "output = Bidirectional(CuDNNLSTM(128,return_sequences = True))(output)\n",
    "attn = MultiHeadAttention(head_num=256, name = 'Multi-head')(output)\n",
    "output = CuDNNLSTM(128,return_sequences=True)(attn)\n",
    "output = CRF(len(label2idx), sparse_target = True)(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/math_ops.py:2509: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 10)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 10, 32) 3040        char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 10, 5)  165         time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, None, 5, 5)   325         time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, None, 4, 5)   485         time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, None, 3, 5)   485         time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, None, 2, 5)   645         time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, None, 256)    165888      time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, None, 5)      0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, None, 5)      0           time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, None, 5)      0           time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, None, 5)      0           time_distributed_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, None, 5)      0           time_distributed_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 281)    0           time_distributed_12[0][0]        \n",
      "                                                                 time_distributed_3[0][0]         \n",
      "                                                                 time_distributed_5[0][0]         \n",
      "                                                                 time_distributed_7[0][0]         \n",
      "                                                                 time_distributed_9[0][0]         \n",
      "                                                                 time_distributed_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 150)    81300       words_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, None, 281)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 431)    0           embedding_1[0][0]                \n",
      "                                                                 time_distributed_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 256)    574464      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Multi-head (MultiHeadAttention) (None, None, 256)    263168      bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)        (None, None, 128)    197632      Multi-head[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, None, 11)     1562        cu_dnnlstm_3[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,289,159\n",
      "Trainable params: 1,207,859\n",
      "Non-trainable params: 81,300\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs = [words_input, char_input], outputs = [output])\n",
    "model.compile(loss = crf_loss, optimizer = 'nadam', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/30\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "1029/1029 [==============================] - 103s 100ms/step\n",
      " \n",
      "Epoch 1/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 2/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 3/30\n",
      "1029/1029 [==============================] - 92s 90ms/step\n",
      " \n",
      "Epoch 4/30\n",
      "1029/1029 [==============================] - 91s 89ms/step\n",
      " \n",
      "Epoch 5/30\n",
      "1029/1029 [==============================] - 91s 89ms/step\n",
      " \n",
      "Epoch 6/30\n",
      "1029/1029 [==============================] - 91s 89ms/step\n",
      " \n",
      "Epoch 7/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 8/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 9/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 10/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 11/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 12/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 13/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 14/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 15/30\n",
      "1029/1029 [==============================] - 91s 89ms/step\n",
      " \n",
      "Epoch 16/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 17/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 18/30\n",
      "1029/1029 [==============================] - 91s 89ms/step\n",
      " \n",
      "Epoch 19/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 20/30\n",
      "1029/1029 [==============================] - 91s 89ms/step\n",
      " \n",
      "Epoch 21/30\n",
      "1029/1029 [==============================] - 91s 89ms/step\n",
      " \n",
      "Epoch 22/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 23/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 24/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 25/30\n",
      "1029/1029 [==============================] - 91s 89ms/step\n",
      " \n",
      "Epoch 26/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 27/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 28/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n",
      "Epoch 29/30\n",
      "1029/1029 [==============================] - 92s 89ms/step\n",
      " \n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch %d/%d\" %(epoch, epochs))\n",
    "    a = Progbar(len(train_batch_size))\n",
    "    for i, batch in enumerate(iterate_minibatches(train_batch, train_batch_size)):\n",
    "        labels, tokens, char = batch\n",
    "        model.train_on_batch([tokens, char], labels)\n",
    "        a.update(i)\n",
    "    a.update(i+1)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../model/ner_glove.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(word2Idx,open(\"../data/pickle_file/word2idx.pkl\", \"wb\"))\n",
    "pickle.dump(char2Idx, open(\"../data/pickle_file/char2idx.pkl\",\"wb\"))\n",
    "pickle.dump(label2idx, open(\"../data/pickle_file/label2idx.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"../model/ner_glove.h5\",\n",
    "                   custom_objects ={'CRF':CRF,\n",
    "                                   'crf_loss':crf_loss,\n",
    "                                   'MultiHeadAttention':MultiHeadAttention})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkaccuracy(predict_label,correct_label):\n",
    "    count = 0\n",
    "    for i in zip(predict_label, correct_label):\n",
    "        if(i[0] != i[1]):\n",
    "            count += 1\n",
    "    if(count == 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65224/65224 [==============================] - 725s 11ms/step\n",
      "Accuracy: 0.9740586287256224\n"
     ]
    }
   ],
   "source": [
    "# Performance on dev dataset\n",
    "predLabels, correctLabels = tag_dataset(test_batch)\n",
    "\n",
    "# Calculate Performance of model on data\n",
    "counter = 0\n",
    "for i in range(len(predLabels)):\n",
    "    counter += checkaccuracy(predLabels[i], correctLabels[i])\n",
    "print(\"Accuracy:\", counter/len(predLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(text):\n",
    "    doc = nlp(text,disable=['ner','tagger','parser'])\n",
    "    doc = [i.text for i in doc]\n",
    "    return doc\n",
    "# tokenized = [tokenizing(i) for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_batch_dataset(text, word2Idx, char2Idx):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            text (str): Input sentence\n",
    "        Return:\n",
    "            A list consist of.\n",
    "            [0]: A list of Token's index in the sentence\n",
    "            [1]: A list of Casing condition of token in the sentence\n",
    "            [2]: A list of list of token's Characters pattern\n",
    "    \"\"\"\n",
    "    word_indices = []\n",
    "    char_indices = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        i = token.text\n",
    "        try:\n",
    "            word_indices.append(word2Idx[i])\n",
    "        except KeyError:\n",
    "            word_indices.append(word2Idx[\"UNK\"])\n",
    "        \n",
    "        tok = []\n",
    "        for j in i:\n",
    "            try:\n",
    "                tok.append(char2Idx[j])\n",
    "            except KeyError:\n",
    "                tok.append(char2Idx[\"UNK\"])\n",
    "        char_indices.append(pad_sequences([tok], maxlen=maxwordlength)[0])\n",
    "        \n",
    "    return [word_indices, char_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_dataset_dev_set(dataset):\n",
    "    predLabels = []\n",
    "    b = Progbar(len(dataset))\n",
    "    for i, data in enumerate(dataset):\n",
    "        tokens, char= data\n",
    "        tokens = np.asarray([tokens])\n",
    "        char = np.asarray([char])\n",
    "        \n",
    "        pred = model.predict([tokens, char], verbose = False)[0]\n",
    "        predLabels.append([np.argmax(i) for i in pred])\n",
    "        b.update(i)\n",
    "    b.update(i+1)\n",
    "    \n",
    "    return predLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_with_unk(sentence_test):\n",
    "    \"\"\"\n",
    "        Replace out-of-vocabulary with unknown (UNK)\n",
    "    \"\"\"\n",
    "    ## Bikin training vocabulary\n",
    "    trainVocab = []\n",
    "    for sentence in trainSentences:\n",
    "        for word, char, label in sentence:\n",
    "            trainVocab.append(word)\n",
    "    \n",
    "    trainVocab = set(trainVocab)\n",
    "    trainVocab.remove('UNK') ## Remove unknown\n",
    "    \n",
    "    ## Split sentence test jadi token\n",
    "    token = sentence_test.split()\n",
    "    new_token = []\n",
    "    for i in token:\n",
    "        if i not in trainVocab:\n",
    "            new_token.append('UNK')\n",
    "        else :\n",
    "            new_token.append(i)\n",
    "            \n",
    "    ## Join all tokens jadi sentence\n",
    "    new_sentence_test = ' '.join(new_token)\n",
    "    \n",
    "    return new_sentence_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_word2idx = open(\"../data/pickle_file/word2idx.pkl\",\"rb\")\n",
    "pickle_char2idx = open(\"../data/pickle_file/char2idx.pkl\",\"rb\")\n",
    "pickle_label2idx = open(\"../data/pickle_file/label2idx.pkl\",\"rb\")\n",
    "word2Idx = pickle.load(pickle_word2idx)\n",
    "char2Idx = pickle.load(pickle_char2idx)\n",
    "label2Idx = pickle.load(pickle_label2idx)\n",
    "idx2label = {v:k for k,v in label2Idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "['BRAND', 'DESCRIPTION', 'DESCRIPTION', 'NUMERIC', 'UOM']\n"
     ]
    }
   ],
   "source": [
    "tmp = \"Abese Kopi Susu Dua Saset\"\n",
    "#tmp = replace_oov_with_unk(tmp)\n",
    "tmp = convert_to_batch_dataset(tmp, word2Idx, char2Idx)\n",
    "ans = tag_dataset_dev_set([tmp])[0]\n",
    "ans = [idx2label[i] for i in ans]\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bolpoin MX Dua Rebu tiga buah\n",
    "#Ji Sam Su Dua Belas tiga batang"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "widgets": {
   "state": {
    "c32dac7758044c1aa849ee962fef82bf": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
