{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from spacy.lang.xx import MultiLanguage\n",
    "nlp = MultiLanguage() #pre-trained model NER\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readline(filename):\n",
    "    \"\"\"\n",
    "        read file\n",
    "        return\n",
    "        format [ ['segelas', 'TYPES'], ['douni', 'PRODUCT], ['parfum', 'PRODUCT'], ['collection', 'PRODUCT'], ['fusion', 'PRODUCT']]\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for i in f:\n",
    "            if(len(i[:-1]) < 2) or i.startswith(\"-DOCSTART-\"):\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "            splits = i[:-1].split(\",\")\n",
    "            sentence.append([splits[0], splits[-1]])\n",
    "        \n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMatrices(sentences, word2Idx, label2Idx, char2Idx):\n",
    "    \"\"\"\n",
    "    Get the matrices for every dataset. Different with normal NER, we do not use casing-features here, because the input \n",
    "    from ASR would be case insensitive\n",
    "        sentences(list) : list of list of words from the input.\n",
    "        word2Idx(dict)   : Word and its index\n",
    "        label2Idx(dict)   : Label and its index\n",
    "        char2Idx(dict)   : Characters and its index\n",
    "        \n",
    "    Output:\n",
    "        dataset(matrix)   :Matrix containing all of the features.\n",
    "    \"\"\"\n",
    "    unknownIdx = word2Idx[\"UNK\"]\n",
    "    paddingIdx = word2Idx[\"PAD\"]\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    word_count = 0\n",
    "    unknown_word_count = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        word_indices = []\n",
    "        char_indices = []\n",
    "        label_indices = []\n",
    "        for word, char, label in sentence:\n",
    "            word_count += 1\n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]\n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "                unknown_word_count += 1\n",
    "            \n",
    "            charIdx = []\n",
    "            for x in char:\n",
    "                try:\n",
    "                    charIdx.append(char2Idx[x])\n",
    "                except KeyError:\n",
    "                    charIdx.append(char2Idx[\"UNK\"])\n",
    "            \n",
    "            word_indices.append(wordIdx)\n",
    "            char_indices.append(charIdx)\n",
    "            label_indices.append(label2Idx[label])\n",
    "            \n",
    "        dataset.append([word_indices, char_indices, label_indices])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCharInformation(sentences):\n",
    "    \"\"\"\n",
    "    Split the word in the sentences from 1 dataset into list of characters:\n",
    "        ex: [[\"S\",\"e\",\"l\",\"a\",\"m\",\"a\",\"t\"],[\"P\",\"a\",\"g\",\"i\"]]\n",
    "    Input:\n",
    "        sentences(list)   : The dataset\n",
    "    \"\"\"    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, data in enumerate(sentence):\n",
    "            chars = [i for i in data[0]]\n",
    "            sentences[i][j] = [data[0], chars, data[1]] ## data[0] is token, chars is chars of token, data[1] is label/entity\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sentences):\n",
    "    \"\"\"\n",
    "    Because the batches already with same length, we are not padding it anymore. We padding the characters instead\n",
    "        ex: [[\"S\",\"e\",\"l\",\"a\",\"m\",\"a\",\"t\"],[\"P\",\"a\",\"g\",\"i\",\"<PAD>\",\"<PAD>\",\"<PAD>\"]]\n",
    "        \n",
    "    Input:\n",
    "        sentence(list)   :The dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    maxwordlength = 15 #15 character max in 1 word\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        sentences[i][1] = pad_sequences(sentences[i][1], maxwordlength, padding='pre',truncating='post')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBatches(data):\n",
    "    \"\"\"\n",
    "    Create the batches for train_on_batch keras.\n",
    "    Input:\n",
    "        data(list)   :The dataset\n",
    "    \"\"\"\n",
    "    l = []\n",
    "    for i in data:\n",
    "        l.append(len(i[0]))\n",
    "    l = set(l)  ## kata\n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    \n",
    "    for i in l:\n",
    "        for batch in data:\n",
    "            if(len(batch[0]) == i):\n",
    "                batches.append(batch)\n",
    "                z+=1\n",
    "        batch_len.append(z)\n",
    "    return batches, batch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(dataset,batch_len):\n",
    "    \"\"\"\n",
    "    Generator for dataset to be yielded during training\n",
    "    Input:\n",
    "        Dataset(list) : The dataset\n",
    "        batch_len(list) :Start and end batch_len\n",
    "    Output:\n",
    "        Array of [labels, tokens, casing, char] features\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        start = 0\n",
    "        for i in batch_len:\n",
    "            tokens = []\n",
    "            char = []\n",
    "            labels = []\n",
    "            data = dataset[start:i]\n",
    "            start = i\n",
    "            for dt in data:\n",
    "                t,ch,l = dt\n",
    "                l = np.expand_dims(l, -1)\n",
    "                tokens.append(t)\n",
    "                char.append(ch)\n",
    "                labels.append(l)\n",
    "            feature = [np.asarray(tokens),np.asarray(char)]\n",
    "            yield feature, np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict data\n",
    "def tag_dataset(dataset):\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "    b = Progbar(len(dataset))\n",
    "    for i, data in enumerate(dataset):\n",
    "        tokens, char, labels = data\n",
    "        tokens = np.asarray([tokens])\n",
    "        char = np.asarray([char])\n",
    "        \n",
    "        pred = model.predict([tokens, char], verbose = False)[0]\n",
    "        pred = pred.argmax(axis = -1) #Predict the classes\n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "        b.update(i)\n",
    "    b.update(i+1)\n",
    "    \n",
    "    return predLabels, correctLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Open the file needed and Convert the word into list of chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences = readline(\"../data/train/train_dataset.csv\")\n",
    "trainSentences = addCharInformation(trainSentences)\n",
    "\n",
    "testSentences = readline(\"../data/test/test_dataset.csv\")\n",
    "testSentences = addCharInformation(testSentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the Label from the dataset, and convert it to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_set = set()\n",
    "words = {}\n",
    "for sentence in trainSentences:\n",
    "    for token, char, label in sentence:\n",
    "        label_set.add(label)\n",
    "        words[token.lower()] = True\n",
    "\n",
    "label2idx = {v:k for k,v in enumerate(sorted(label_set))}\n",
    "idx2label = {v:k for k,v in label2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Mittens Word Embedding (Extensions for GloVe model). Its retrofitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load word2idx pickle\n",
    "word2idx_mittens = pickle.load(open(\"../data/pickle_file/word2idx_mittens.pkl\",\"rb\"))\n",
    "word2Idx = {i[0]:i[1] for i in word2idx_mittens}\n",
    "\n",
    "## Load mittens embedding\n",
    "embedding_file_path = '../model/mittens_embedding.pkl'\n",
    "wordEmbeddings = pickle.load(open(embedding_file_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## n-dimensional vector of word embeddings\n",
    "n_dim = wordEmbeddings.shape[1]\n",
    "\n",
    "word2Idx.update({'PAD': len(word2Idx)})\n",
    "wordEmbeddings = np.concatenate((wordEmbeddings, np.zeros((1, n_dim))))\n",
    "word2Idx.update({'UNK': len(word2Idx)})\n",
    "wordEmbeddings = np.concatenate((wordEmbeddings, np.random.uniform(-0.25*10**-3, 0.25*10**-3,n_dim).reshape((1,n_dim))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Get list of characters and its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2Idx = {\"PAD\":0, \"UNK\":1}\n",
    "for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|\":\n",
    "    char2Idx[c] = len(char2Idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set = createMatrices(trainSentences, \n",
    "               word2Idx = word2Idx, \n",
    "               label2Idx = label2idx,\n",
    "               char2Idx = char2Idx)\n",
    "\n",
    "test_set = createMatrices(testSentences, \n",
    "               word2Idx = word2Idx, \n",
    "               label2Idx = label2idx,\n",
    "               char2Idx = char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = padding(train_set)\n",
    "test_set = padding(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(train_set, train_size = 0.9, random_state=42, shuffle=True)\n",
    "X_val, X_test = train_test_split(X_test, train_size= 0.5, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_batch, train_batch_len = createBatches(X_train)\n",
    "val_batch, val_batch_len = createBatches(X_val)\n",
    "test_batch, test_batch_len = createBatches(X_test)\n",
    "real_test_batch, real_test_batch_len = createBatches(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2032, 128107, 401351, 663899, 908055, 1087245, 1185080, 1198715, 1203313, 1206735]\n",
      "[109, 7064, 22184, 36883, 50449, 60263, 65823, 66614, 66854, 67041]\n",
      "[130, 7100, 22508, 37036, 50572, 60534, 65929, 66632, 66878, 67041]\n",
      "[156, 3634, 22854, 55564, 83042, 101962, 121306, 127466]\n"
     ]
    }
   ],
   "source": [
    "print(train_batch_len)\n",
    "print(val_batch_len)\n",
    "print(test_batch_len)\n",
    "print(real_test_batch_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_batch_size = []\n",
    "val_batch_size = []\n",
    "test_batch_size = []\n",
    "real_test_batch_size = []\n",
    "\n",
    "## Change training batch size\n",
    "for val in range(batch_size, max(train_batch_len), batch_size):\n",
    "    train_batch_size.append(val)\n",
    "for val in train_batch_len:\n",
    "    train_batch_size.append(val)\n",
    "    \n",
    "train_batch_size.sort()\n",
    "\n",
    "## Change validation batch size\n",
    "for val in range(batch_size, max(val_batch_len), batch_size):\n",
    "    val_batch_size.append(val)\n",
    "for val in val_batch_len:\n",
    "    val_batch_size.append(val)\n",
    "    \n",
    "val_batch_size.sort()\n",
    "\n",
    "## Change testing batch size\n",
    "for val in range(batch_size, max(test_batch_len), batch_size):\n",
    "    test_batch_size.append(val)\n",
    "for val in test_batch_len:\n",
    "    test_batch_size.append(val)\n",
    "\n",
    "test_batch_size.sort()\n",
    "\n",
    "## Change real testing batch size\n",
    "for val in range(batch_size, max(real_test_batch_len), batch_size):\n",
    "    real_test_batch_size.append(val)\n",
    "for val in real_test_batch_len:\n",
    "    real_test_batch_size.append(val)\n",
    "\n",
    "real_test_batch_size.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dropout, TimeDistributed, Conv1D, Dense, multiply, CuDNNLSTM, GlobalAveragePooling1D\n",
    "from keras.layers import Concatenate, MaxPooling1D, GlobalMaxPooling1D, Flatten, Bidirectional, LSTM, ThresholdedReLU, BatchNormalization\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import plot_model,Progbar\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_multi_head import MultiHeadAttention\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxwordlength = 15\n",
    "dropout_p = 0.5\n",
    "char_embedding_dim = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embed(wordEmbeddings):\n",
    "    ## Input Layer\n",
    "    words_input = Input(shape = (None,), dtype = np.int32, name = 'words_input')\n",
    "    ## Embedding Layer\n",
    "    words = Embedding(input_dim = wordEmbeddings.shape[0],\n",
    "                      output_dim = wordEmbeddings.shape[1],\n",
    "                      weights = [wordEmbeddings],\n",
    "                      trainable = False)(words_input)\n",
    "    return words_input, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_embed(maxwordlength, char2Idx, char_embedding_dim):\n",
    "    ## Input Layer\n",
    "    chars_input = Input(shape = (None, maxwordlength, ), name = 'chars_input')\n",
    "    ## Embedding Layer\n",
    "    chars = TimeDistributed(Embedding(input_dim = len(char2Idx),\n",
    "                  output_dim = char_embedding_dim,\n",
    "                  embeddings_initializer = RandomUniform(minval=-0.5, maxval=0.5)))(chars_input)\n",
    "    return chars_input, chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dropout_p, chars, words):\n",
    "    ## Convolution layer\n",
    "    conv = TimeDistributed(Dropout(dropout_p))(chars)\n",
    "    conv = TimeDistributed(Conv1D(filters = 30,\n",
    "                                  kernel_size = 3,\n",
    "                                  padding = 'same',\n",
    "                                  activation = 'tanh',\n",
    "                                  strides = 1))(conv)\n",
    "    conv = TimeDistributed(MaxPooling1D(maxwordlength))(conv)\n",
    "    ## Flatten\n",
    "    conv = TimeDistributed(Flatten())(conv)\n",
    "    chars = TimeDistributed(Dropout(dropout_p))(conv)\n",
    "    ## Concatenate words embed and char-representation\n",
    "    output = Concatenate()([words, chars])\n",
    "    ## Bi-LSTM\n",
    "    output = Bidirectional(CuDNNLSTM(200, kernel_initializer='random_uniform',\n",
    "                           bias_initializer='zeros', return_sequences = True))(output)\n",
    "    ## Dropout\n",
    "    output = Dropout(dropout_p)(output)\n",
    "    ## MultiHeadAttention\n",
    "    '''Multi-head attention allows the model to jointly attend to information from different\n",
    "    representation subspaces at different positions''' \n",
    "    attn = MultiHeadAttention(head_num=8, name = 'Multi-head')(output)\n",
    "    ## LSTM\n",
    "    output = CuDNNLSTM(200, kernel_initializer='random_uniform',\n",
    "             bias_initializer='zeros', return_sequences = True)(attn)\n",
    "    ## Batch Normalization\n",
    "    output = BatchNormalization()(output)\n",
    "    ## CRF\n",
    "    crf = CRF(len(label2idx), sparse_target=True)\n",
    "    output = crf(output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_input, words = get_word_embed(wordEmbeddings)\n",
    "chars_input, chars = get_char_embed(maxwordlength, char2Idx, char_embedding_dim)\n",
    "output = get_model(dropout_p, chars, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Optimizer\n",
    "nadam = optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model = Model(inputs = [words_input, chars_input], outputs = [output])\n",
    "model.compile(loss = crf_loss, optimizer = nadam, metrics=[crf_accuracy])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/50\n",
      "4723/4723 [==============================] - 469s 99ms/step - loss: -0.3143 - crf_accuracy: 0.9842 - val_loss: -0.1233 - val_crf_accuracy: 0.8622\n",
      "Epoch 2/50\n",
      " 156/4723 [..............................] - ETA: 5:52 - loss: -0.8296 - crf_accuracy: 0.9571"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-88f17f567703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m history = model.fit_generator(train_generator, steps_per_epoch=len(train_batch_size), epochs=epochs,\n\u001b[1;32m      9\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                     validation_data=test_generator, validation_steps=len(real_test_batch_size))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    224\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mbatch_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mbatch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \"\"\"\n\u001b[1;32m    365\u001b[0m         \u001b[0;31m# For backwards compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;31m# will be handled by on_epoch_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Generator\n",
    "train_generator = iterate_minibatches(train_batch, train_batch_size)\n",
    "val_generator = iterate_minibatches(val_batch, val_batch_size)\n",
    "test_generator = iterate_minibatches(real_test_batch, real_test_batch_size)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_crf_accuracy', mode='max', verbose=1, patience=10)\n",
    "epochs = 50\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=len(train_batch_size), epochs=epochs,\n",
    "                    callbacks=[early_stopping_callback],\n",
    "                    validation_data=test_generator, validation_steps=len(real_test_batch_size), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../model/ner_order.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot model accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['crf_accuracy'])\n",
    "plt.plot(history.history['val_crf_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(word2Idx,open(\"../data/pickle_file/word2idx.pkl\", \"wb\"))\n",
    "pickle.dump(char2Idx, open(\"../data/pickle_file/char2idx.pkl\",\"wb\"))\n",
    "pickle.dump(label2idx, open(\"../data/pickle_file/label2idx.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"../model/ner_order.h5\",\n",
    "                   custom_objects ={'CRF':CRF,\n",
    "                                   'crf_loss':crf_loss,\n",
    "                                   'crf_accuracy':crf_accuracy,\n",
    "                                   'MultiHeadAttention':MultiHeadAttention})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkaccuracy(predict_label,correct_label):\n",
    "    count = 0\n",
    "    for i in zip(predict_label, correct_label):\n",
    "        if(i[0] != i[1]):\n",
    "            count += 1\n",
    "    if(count == 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on dev dataset\n",
    "predLabels, correctLabels = tag_dataset(test_batch)\n",
    "\n",
    "# Calculate Performance of model on data\n",
    "counter = 0\n",
    "for i in range(len(predLabels)):\n",
    "    counter += checkaccuracy(predLabels[i], correctLabels[i])\n",
    "print(\"Accuracy:\", counter/len(predLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning using Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_with(verbose, wordEmbeddings, maxwordlength, char2Idx, char_embedding_dim, train_generator, \n",
    "             train_batch_size, test_generator, real_test_batch_size, dropout_p, lr):\n",
    "    \n",
    "    # Create the embedding\n",
    "    words_input, words = get_word_embed(wordEmbeddings)\n",
    "    chars_input, chars = get_char_embed(maxwordlength, char2Idx, char_embedding_dim)\n",
    "    \n",
    "    # Create the model using a specified hyperparameters.\n",
    "    model = get_model(dropout_p, chars, words)\n",
    "\n",
    "    # Train the model for a specified number of epochs.\n",
    "    optimizer = optimizers.Nadam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "    model = Model(inputs = [words_input, chars_input], outputs = [model])\n",
    "    model.compile(loss = crf_loss, optimizer = optimizer, metrics=[crf_accuracy])\n",
    "\n",
    "    # Train the model with the train dataset.\n",
    "    print('Starting--')\n",
    "    epochs = 10\n",
    "    model.fit_generator(train_generator, steps_per_epoch=len(train_batch_size), epochs=epochs)\n",
    "\n",
    "    # Evaluate the model with the test dataset.\n",
    "    score = model.evaluate_generator(test_generator, steps=len(real_test_batch_size))\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    # Return the accuracy.\n",
    "    return score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "verbose = 1\n",
    "fit_with_partial = partial(fit_with, verbose, wordEmbeddings, maxwordlength, char2Idx,\n",
    "                           char_embedding_dim, train_generator,train_batch_size, test_generator,\n",
    "                           real_test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.util import load_logs\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'dropout_p': (0.35, 0.7), 'lr': (0.001, 0.01)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=fit_with_partial,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "logger = JSONLogger(path=\"../model/logs.json\")\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "optimizer.maximize(init_points=3, n_iter=50)\n",
    "\n",
    "# New optimizer is loaded with previously seen points\n",
    "load_logs(optimizer, logs=[\"../model/logs.json\"])\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "print(\"Result Fine Tuning : \", optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "state": {
    "c32dac7758044c1aa849ee962fef82bf": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
