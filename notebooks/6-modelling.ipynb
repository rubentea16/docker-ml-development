{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from spacy.lang.xx import MultiLanguage\n",
    "nlp = MultiLanguage() #pre-trained model NER\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readline(filename):\n",
    "    \"\"\"\n",
    "        read file\n",
    "        return\n",
    "        format [ ['segelas', 'TYPES'], ['douni', 'PRODUCT], ['parfum', 'PRODUCT'], ['collection', 'PRODUCT'], ['fusion', 'PRODUCT']]\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for i in f:\n",
    "            if(len(i[:-1]) < 2) or i.startswith(\"-DOCSTART-\"):\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "            splits = i[:-1].split(\",\")\n",
    "            sentence.append([splits[0], splits[-1]])\n",
    "        \n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMatrices(sentences, word2Idx, label2Idx, char2Idx):\n",
    "    \"\"\"\n",
    "    Get the matrices for every dataset. Different with normal NER, we do not use casing-features here, because the input \n",
    "    from ASR would be case insensitive\n",
    "        sentences(list) : list of list of words from the input.\n",
    "        word2Idx(dict)   : Word and its index\n",
    "        label2Idx(dict)   : Label and its index\n",
    "        char2Idx(dict)   : Characters and its index\n",
    "        \n",
    "    Output:\n",
    "        dataset(matrix)   :Matrix containing all of the features.\n",
    "    \"\"\"\n",
    "    unknownIdx = word2Idx[\"UNK\"]\n",
    "    paddingIdx = word2Idx[\"PAD\"]\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    word_count = 0\n",
    "    unknown_word_count = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        word_indices = []\n",
    "        char_indices = []\n",
    "        label_indices = []\n",
    "        for word, char, label in sentence:\n",
    "            word_count += 1\n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]\n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "                unknown_word_count += 1\n",
    "            \n",
    "            charIdx = []\n",
    "            for x in char:\n",
    "                try:\n",
    "                    charIdx.append(char2Idx[x])\n",
    "                except KeyError:\n",
    "                    charIdx.append(char2Idx[\"UNK\"])\n",
    "            \n",
    "            word_indices.append(wordIdx)\n",
    "            char_indices.append(charIdx)\n",
    "            label_indices.append(label2Idx[label])\n",
    "            \n",
    "        dataset.append([word_indices, char_indices, label_indices])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCharInformation(sentences):\n",
    "    \"\"\"\n",
    "    Split the word in the sentences from 1 dataset into list of characters:\n",
    "        ex: [[\"S\",\"e\",\"l\",\"a\",\"m\",\"a\",\"t\"],[\"P\",\"a\",\"g\",\"i\"]]\n",
    "    Input:\n",
    "        sentences(list)   : The dataset\n",
    "    \"\"\"    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, data in enumerate(sentence):\n",
    "            chars = [i for i in data[0]]\n",
    "            sentences[i][j] = [data[0], chars, data[1]] ## data[0] is token, chars is chars of token, data[1] is label/entity\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sentences):\n",
    "    \"\"\"\n",
    "    Because the batches already with same length, we are not padding it anymore. We padding the characters instead\n",
    "        ex: [[\"S\",\"e\",\"l\",\"a\",\"m\",\"a\",\"t\"],[\"P\",\"a\",\"g\",\"i\",\"<PAD>\",\"<PAD>\",\"<PAD>\"]]\n",
    "        \n",
    "    Input:\n",
    "        sentence(list)   :The dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    maxwordlength = 15 #15 character max in 1 word\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        sentences[i][1] = pad_sequences(sentences[i][1], maxwordlength, padding='pre',truncating='post')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBatches(data):\n",
    "    \"\"\"\n",
    "    Create the batches for train_on_batch keras.\n",
    "    Input:\n",
    "        data(list)   :The dataset\n",
    "    \"\"\"\n",
    "    l = []\n",
    "    for i in data:\n",
    "        l.append(len(i[0]))\n",
    "    l = set(l)  ## kata\n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    \n",
    "    for i in l:\n",
    "        for batch in data:\n",
    "            if(len(batch[0]) == i):\n",
    "                batches.append(batch)\n",
    "                z+=1\n",
    "        batch_len.append(z)\n",
    "    return batches, batch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(dataset,batch_len):\n",
    "    \"\"\"\n",
    "    Generator for dataset to be yielded during training\n",
    "    Input:\n",
    "        Dataset(list) : The dataset\n",
    "        batch_len(list) :Start and end batch_len\n",
    "    Output:\n",
    "        Array of [labels, tokens, casing, char] features\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        start = 0\n",
    "        for i in batch_len:\n",
    "            tokens = []\n",
    "            char = []\n",
    "            labels = []\n",
    "            data = dataset[start:i]\n",
    "            start = i\n",
    "            for dt in data:\n",
    "                t,ch,l = dt\n",
    "                l = np.expand_dims(l, -1)\n",
    "                tokens.append(t)\n",
    "                char.append(ch)\n",
    "                labels.append(l)\n",
    "            feature = [np.asarray(tokens),np.asarray(char)]\n",
    "            yield feature, np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict data\n",
    "def tag_dataset(dataset):\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "    b = Progbar(len(dataset))\n",
    "    for i, data in enumerate(dataset):\n",
    "        tokens, char, labels = data\n",
    "        tokens = np.asarray([tokens])\n",
    "        char = np.asarray([char])\n",
    "        \n",
    "        pred = model.predict([tokens, char], verbose = False)[0]\n",
    "        pred = pred.argmax(axis = -1) #Predict the classes\n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "        b.update(i)\n",
    "    b.update(i+1)\n",
    "    \n",
    "    return predLabels, correctLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Open the file needed and Convert the word into list of chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences = readline(\"../data/train/train_dataset.csv\")\n",
    "trainSentences = addCharInformation(trainSentences)\n",
    "\n",
    "testSentences = readline(\"../data/test/test_dataset.csv\")\n",
    "testSentences = addCharInformation(testSentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the Label from the dataset, and convert it to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_set = set()\n",
    "words = {}\n",
    "for sentence in trainSentences:\n",
    "    for token, char, label in sentence:\n",
    "        label_set.add(label)\n",
    "        words[token.lower()] = True\n",
    "\n",
    "label2idx = {v:k for k,v in enumerate(sorted(label_set))}\n",
    "idx2label = {v:k for k,v in label2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. GloVe Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load word2idx pickle\n",
    "word2idx_corpus = pickle.load(open(\"../data/pickle_file/word2idx_corpus.pkl\",\"rb\"))\n",
    "word2Idx = {i[0]:i[1] for i in word2idx_corpus}\n",
    "\n",
    "## Load GloVe embedding\n",
    "embedding_file_path = '../model/glove_embedding.pkl'\n",
    "wordEmbeddings = pickle.load(open(embedding_file_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## n-dimensional vector of word embeddings\n",
    "n_dim = wordEmbeddings.shape[1]\n",
    "\n",
    "word2Idx.update({'PAD': len(word2Idx)})\n",
    "wordEmbeddings = np.concatenate((wordEmbeddings, np.zeros((1, n_dim))))\n",
    "word2Idx.update({'UNK': len(word2Idx)})\n",
    "wordEmbeddings = np.concatenate((wordEmbeddings, np.random.uniform(-0.25*10**-3, 0.25*10**-3,n_dim).reshape((1,n_dim))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Get list of characters and its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2Idx = {\"PAD\":0, \"UNK\":1}\n",
    "for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|\":\n",
    "    char2Idx[c] = len(char2Idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set = createMatrices(trainSentences, \n",
    "               word2Idx = word2Idx, \n",
    "               label2Idx = label2idx,\n",
    "               char2Idx = char2Idx)\n",
    "\n",
    "test_set = createMatrices(testSentences, \n",
    "               word2Idx = word2Idx, \n",
    "               label2Idx = label2idx,\n",
    "               char2Idx = char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = padding(train_set)\n",
    "test_set = padding(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(train_set, train_size = 0.8, random_state=42, shuffle=True)\n",
    "X_val, X_test = train_test_split(X_test, train_size= 0.5, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_batch, train_batch_len = createBatches(X_train)\n",
    "val_batch, val_batch_len = createBatches(X_val)\n",
    "test_batch, test_batch_len = createBatches(X_test)\n",
    "real_test_batch, real_test_batch_len = createBatches(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1795, 114003, 356609, 589916, 807034, 966387, 1053362, 1065542, 1069644, 1072653]\n",
      "[254, 14115, 44811, 74023, 101072, 120841, 131748, 133202, 133675, 134082]\n",
      "[222, 14153, 44623, 73879, 100970, 120814, 131722, 133217, 133726, 134082]\n",
      "[156, 3634, 22854, 55564, 83042, 101962, 121306, 127466]\n"
     ]
    }
   ],
   "source": [
    "print(train_batch_len)\n",
    "print(val_batch_len)\n",
    "print(test_batch_len)\n",
    "print(real_test_batch_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_batch_size = []\n",
    "val_batch_size = []\n",
    "test_batch_size = []\n",
    "real_test_batch_size = []\n",
    "\n",
    "## Change training batch size\n",
    "for val in range(batch_size, max(train_batch_len), batch_size):\n",
    "    train_batch_size.append(val)\n",
    "for val in train_batch_len:\n",
    "    train_batch_size.append(val)\n",
    "    \n",
    "train_batch_size.sort()\n",
    "\n",
    "## Change validation batch size\n",
    "for val in range(batch_size, max(val_batch_len), batch_size):\n",
    "    val_batch_size.append(val)\n",
    "for val in val_batch_len:\n",
    "    val_batch_size.append(val)\n",
    "    \n",
    "val_batch_size.sort()\n",
    "\n",
    "## Change testing batch size\n",
    "for val in range(batch_size, max(test_batch_len), batch_size):\n",
    "    test_batch_size.append(val)\n",
    "for val in test_batch_len:\n",
    "    test_batch_size.append(val)\n",
    "\n",
    "test_batch_size.sort()\n",
    "\n",
    "## Change real testing batch size\n",
    "for val in range(batch_size, max(real_test_batch_len), batch_size):\n",
    "    real_test_batch_size.append(val)\n",
    "for val in real_test_batch_len:\n",
    "    real_test_batch_size.append(val)\n",
    "\n",
    "real_test_batch_size.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dropout, TimeDistributed, Conv1D, Dense, multiply, CuDNNLSTM, GlobalAveragePooling1D\n",
    "from keras.layers import Concatenate, MaxPooling1D, GlobalMaxPooling1D, Flatten, Bidirectional, LSTM, ThresholdedReLU, BatchNormalization\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import plot_model,Progbar\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_multi_head import MultiHeadAttention\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxwordlength = 15\n",
    "dropout_p = 0.5\n",
    "char_embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embed(wordEmbeddings):\n",
    "    ## Input Layer\n",
    "    words_input = Input(shape = (None,), dtype = np.int32, name = 'words_input')\n",
    "    ## Embedding Layer\n",
    "    words = Embedding(input_dim = wordEmbeddings.shape[0],\n",
    "                      output_dim = wordEmbeddings.shape[1],\n",
    "                      weights = [wordEmbeddings],\n",
    "                      trainable = False)(words_input)\n",
    "    return words_input, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_embed(maxwordlength, char2Idx, char_embedding_dim):\n",
    "    ## Input Layer\n",
    "    chars_input = Input(shape = (None, maxwordlength, ), name = 'chars_input')\n",
    "    ## Embedding Layer\n",
    "    chars = TimeDistributed(Embedding(input_dim = len(char2Idx),\n",
    "                  output_dim = char_embedding_dim,\n",
    "                  embeddings_initializer = RandomUniform(minval=-0.5, maxval=0.5)))(chars_input)\n",
    "    return chars_input, chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_character_block(block, dropout=0.5, filters=[64, 100], kernel_size=[3, 3], \n",
    "                           pool_size=[2, 2], padding='valid', activation='relu', \n",
    "                           kernel_initializer='glorot_normal'):\n",
    "    \n",
    "    for i in range(len(filters)):\n",
    "        block = TimeDistributed(Conv1D(filters=filters[i], kernel_size=kernel_size[i],\n",
    "                       padding=padding, activation=activation,\n",
    "                       kernel_initializer=kernel_initializer))(block)\n",
    "        block = TimeDistributed(Dropout(dropout))(block)\n",
    "        block = TimeDistributed(MaxPooling1D(pool_size=pool_size[i]))(block)\n",
    "        \n",
    "    block = TimeDistributed(GlobalMaxPooling1D())(block)\n",
    "    block = TimeDistributed(Flatten())(block)\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dropout_p, chars, words):\n",
    "    ## Character embedding use CNN\n",
    "    chars = build_character_block(chars)\n",
    "    ## Concatenate words-embedding and char-embedding\n",
    "    output = Concatenate()([words, chars])\n",
    "    ## Bi-LSTM\n",
    "    output = Bidirectional(CuDNNLSTM(200, kernel_initializer='random_uniform',\n",
    "                           bias_initializer='zeros', return_sequences = True))(output)\n",
    "    ## Dropout\n",
    "    output = Dropout(dropout_p)(output)\n",
    "    ## MultiHeadAttention\n",
    "    '''Multi-head attention allows the model to jointly attend to information from different\n",
    "    representation subspaces at different positions''' \n",
    "    attn = MultiHeadAttention(head_num=8, name = 'Multi-head')(output)\n",
    "    ## Batch Normalization\n",
    "    output = BatchNormalization()(attn)\n",
    "    ## CRF\n",
    "    crf = CRF(len(label2idx), sparse_target=True)\n",
    "    output = crf(output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_input, words = get_word_embed(wordEmbeddings)\n",
    "chars_input, chars = get_char_embed(maxwordlength, char2Idx, char_embedding_dim)\n",
    "output = get_model(dropout_p, chars, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Optimizer\n",
    "nadam = optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model = Model(inputs = [words_input, chars_input], outputs = [output])\n",
    "model.compile(loss = crf_loss, optimizer = nadam, metrics=[crf_accuracy])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Generator\n",
    "train_generator = iterate_minibatches(train_batch, train_batch_size)\n",
    "val_generator = iterate_minibatches(val_batch, val_batch_size)\n",
    "test_generator = iterate_minibatches(real_test_batch, real_test_batch_size)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_crf_accuracy', mode='max', verbose=1, patience=10)\n",
    "epochs = 50\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=len(train_batch_size), epochs=epochs,\n",
    "                    callbacks=[early_stopping_callback],\n",
    "                    validation_data=val_generator, validation_steps=len(val_batch_size), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../model/ner_order_v4.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot model accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['crf_accuracy'])\n",
    "plt.plot(history.history['val_crf_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(word2Idx,open(\"../data/pickle_file/word2idx.pkl\", \"wb\"))\n",
    "pickle.dump(char2Idx, open(\"../data/pickle_file/char2idx.pkl\",\"wb\"))\n",
    "pickle.dump(label2idx, open(\"../data/pickle_file/label2idx.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"../model/ner_order_v4.h5\",\n",
    "                   custom_objects ={'CRF':CRF,\n",
    "                                   'crf_loss':crf_loss,\n",
    "                                   'crf_accuracy':crf_accuracy,\n",
    "                                   'MultiHeadAttention':MultiHeadAttention})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkaccuracy_word(predict_label,correct_label):\n",
    "    counter = 0\n",
    "    idx_wrong_pred = []\n",
    "    \n",
    "    for idx_sentence in tqdm(range(len(predict_label))):\n",
    "        for idx_word in range(len(predict_label[idx_sentence])):\n",
    "            if (predict_label[idx_sentence][idx_word]) == (correct_label[idx_sentence][idx_word]):\n",
    "                counter += 1\n",
    "            else :\n",
    "                idx_wrong_pred.append(idx_sentence)\n",
    "                \n",
    "    total_word = 0\n",
    "    for i in predict_label:\n",
    "        total_word += len(i)\n",
    "        \n",
    "    return (counter*100/total_word), idx_wrong_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkaccuracy_sent(predict_label, correct_label):\n",
    "    count = 0\n",
    "    for idx in range(len(predict_label)):\n",
    "        if (predict_label[idx] == correct_label[idx]).all() :\n",
    "            count += 1\n",
    "    return (count*100/len(predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134082/134082 [==============================] - 1000s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2038a9de1844cf39d797725185db31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=134082.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy per sentence : 89.62724303038439\n",
      "Accuracy per word : 97.61495436549139\n"
     ]
    }
   ],
   "source": [
    "# Performance on dev dataset\n",
    "predLabels, correctLabels = tag_dataset(test_batch)\n",
    "\n",
    "# Calculate Performance of model on data\n",
    "count_word, idx_wrong_pred = checkaccuracy_word(predLabels, correctLabels)\n",
    "count_sent = checkaccuracy_sent(predLabels, correctLabels)\n",
    "print(\"Accuracy per sentence :\", count_sent)\n",
    "print(\"Accuracy per word :\", count_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning using Bayesian Optimization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def fit_with(verbose, wordEmbeddings, maxwordlength, char2Idx, char_embedding_dim, train_generator, \n",
    "             train_batch_size, test_generator, real_test_batch_size, dropout_p, lr):\n",
    "    \n",
    "    # Create the embedding\n",
    "    words_input, words = get_word_embed(wordEmbeddings)\n",
    "    chars_input, chars = get_char_embed(maxwordlength, char2Idx, char_embedding_dim)\n",
    "    \n",
    "    # Create the model using a specified hyperparameters.\n",
    "    model = get_model(dropout_p, chars, words)\n",
    "\n",
    "    # Train the model for a specified number of epochs.\n",
    "    optimizer = optimizers.Nadam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "    model = Model(inputs = [words_input, chars_input], outputs = [model])\n",
    "    model.compile(loss = crf_loss, optimizer = optimizer, metrics=[crf_accuracy])\n",
    "\n",
    "    # Train the model with the train dataset.\n",
    "    print('Starting--')\n",
    "    epochs = 10\n",
    "    model.fit_generator(train_generator, steps_per_epoch=len(train_batch_size), epochs=epochs)\n",
    "\n",
    "    # Evaluate the model with the test dataset.\n",
    "    score = model.evaluate_generator(test_generator, steps=len(real_test_batch_size))\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    # Return the accuracy.\n",
    "    return score[1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from functools import partial\n",
    "\n",
    "verbose = 1\n",
    "fit_with_partial = partial(fit_with, verbose, wordEmbeddings, maxwordlength, char2Idx,\n",
    "                           char_embedding_dim, train_generator,train_batch_size, test_generator,\n",
    "                           real_test_batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.util import load_logs\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'dropout_p': (0.35, 0.7), 'lr': (0.001, 0.01)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=fit_with_partial,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "logger = JSONLogger(path=\"../model/logs.json\")\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "optimizer.maximize(init_points=3, n_iter=50)\n",
    "\n",
    "# New optimizer is loaded with previously seen points\n",
    "load_logs(optimizer, logs=[\"../model/logs.json\"])\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "print(\"Result Fine Tuning : \", optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "state": {
    "c32dac7758044c1aa849ee962fef82bf": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
